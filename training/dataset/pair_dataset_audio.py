'''
# author: H.
# email: H.
# date: ..

The code is designed for scenarios such as disentanglement-based methods where it is necessary to ensure an equal number of positive and negative samples.
'''

import torch
import random
import numpy as np

from torch.autograd import Variable
from torch.utils import data
from torchvision import transforms
import soundfile as sf

def pad(x, max_len=64600):
    
    x_len = x.shape[0]
    if x_len >= max_len:
        return x[:max_len]
    # need to pad
    num_repeats = int(max_len / x_len)+1
    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]
    
    return padded_x

class pairDatasetAudio(data.Dataset):
    def __init__(self, config=None, mode='train'):
        """
        NOTE:
        the train_fl format:
            audiopath,label
            when label is value 0, this means this audio is real voice.
            when label is value 1,2,..., this means this audio is fake,and generated by the GAN1, GAN2,...
        """
        # Get real and fake audio lists
        # Fix the label of real audios to be 0 and fake audios to be 1
        self.audio_list = []
        self.label_list = []
        
        if mode == 'train':
            anno_fl = config['train_file']
        elif mode == 'dev':
            anno_fl = config['dev_file']
        elif mode == 'test':
            anno_fl = config['test_file']
            
        with open(anno_fl) as f:
            for line in f:
                aup, lbl, lblname = line.strip().split(',')
                self.audio_list.append(aup)
                self.label_list.append(int(lbl))

        self.fake_audlist = [(aud, label, 1) for aud, label in zip(self.audio_list, self.label_list) if label != 0]
        self.real_audlist = [(aud, label, 0) for aud, label in zip(self.audio_list, self.label_list) if label == 0]
        
        self.transforms = transforms.Compose([
            lambda x: pad(x, max_len=1024*8*8),
            lambda x: torch.Tensor(x)

        ])
        self.data_dict = {
            'audio': self.audio_list, 
            'label': self.label_list, 
        }        
    def load_audio(self, aup):
        data_x, _ = sf.read(aup)
        data_x = self.transforms(data_x)
        return data_x

    def __getitem__(self, index):
        # Get the fake and real audio paths and labels
        fake_audio_path, fake_spe_label, fake_label = self.fake_audlist[index]
        real_index = random.randint(0, len(self.real_audlist) - 1)  # Randomly select a real audio
        real_audio_path, real_spe_label, real_label = self.real_audlist[real_index]

        # Load the fake and real audios
        fake_audio = self.load_audio(fake_audio_path)
        real_audio = self.load_audio(real_audio_path)

        # fake_audio = np.array(fake_audio)  # Convert to numpy array for data augmentation
        # real_audio = np.array(real_audio)  # Convert to numpy array for data augmentation
        # print(f'fake_audio: {fake_audio.shape}, real_audio: {real_audio.shape}')
        return {"fake": (fake_audio, fake_label, fake_spe_label), 
                "real": (real_audio, real_label, real_spe_label)}

    def __len__(self):
        return len(self.fake_audlist)

    @staticmethod
    def collate_fn(batch):
        """
        Collate a batch of data points.

        Args:
            batch (list): A list of tuples containing the audio tensor, the label tensor,
                        the landmark tensor, and the mask tensor.

        Returns:
            A tuple containing the audio tensor, the label tensor, the landmark tensor,
            and the mask tensor.
        """
        # Separate the audio, label, landmark, and mask tensors for fake and real data
        fake_audios, fake_labels, fake_spe_labels = zip(*[data["fake"] for data in batch])
        real_audios, real_labels, real_spe_labels = zip(*[data["real"] for data in batch])

        # Stack the audio, label, landmark, and mask tensors for fake and real data
        # torch.save(fake_audios, "/face/hnren/3.SSL/codes/research_proposal/audio_deepfake_detection/codes/DeepfakeBench/dbg/fake_audios.pth")
        # import sys;sys.exit() 

        fake_audios = torch.stack(fake_audios, dim=0)
        fake_labels = torch.LongTensor(fake_labels)
        fake_spe_labels = torch.LongTensor(fake_spe_labels)
        real_audios = torch.stack(real_audios, dim=0)
        real_labels = torch.LongTensor(real_labels)
        real_spe_labels = torch.LongTensor(real_spe_labels)

        # Combine the fake and real tensors and create a dictionary of the tensors
        audios = torch.cat([real_audios, fake_audios], dim=0)
        labels = torch.cat([real_labels, fake_labels], dim=0)
        spe_labels = torch.cat([real_spe_labels, fake_spe_labels], dim=0)
        
        data_dict = {
            'audio': audios,
            'label': labels,
            'label_spe': spe_labels,
        }
        return data_dict
    


